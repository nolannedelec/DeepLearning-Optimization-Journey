# CIFAR-10 CNN : Parcours d'Optimisation et Apprentissage

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)

## üìå Pr√©sentation du Projet
Ce d√©p√¥t documente l'√©volution d'un mod√®le de classification d'images (Dataset CIFAR-10) d√©velopp√© avec **PyTorch**. 

> [!IMPORTANT]
> Ce projet est une **d√©marche p√©dagogique**. L'objectif n'√©tait pas d'atteindre un score de pointe par simple copie de mod√®les existants, mais de comprendre l'impact r√©el de chaque brique technologique (Batch Normalization, Data Augmentation, Dropout) en les impl√©mentant de mani√®re it√©rative.

## √âtapes de l'Optimisation

Le mod√®le a √©volu√© d'une architecture basique vers une version optimis√©e incluant :

1. **Architecture CNN Profonde** : Passage √† un syst√®me de 4 blocs convolutifs (32 et 64 filtres) pour extraire des caract√©ristiques complexes.
2. **Batch Normalization (BN)** : L'ajout de couches `BatchNorm2d` apr√®s chaque convolution a √©t√© le point de bascule du projet, stabilisant massivement l'apprentissage.
3. **Optimisation de la D√©cision (Bottleneck)** : Ajout d'une troisi√®me couche lin√©aire (passage de 512 √† 64 neurones) pour forcer le mod√®le √† synth√©tiser ses connaissances.
4. **Data Augmentation** : Test de transformations (flips, rotations, jitter) pour am√©liorer la robustesse globale.
5. **R√©gularisation (Dropout)** : Impl√©mentation de Dropout (0.3) pour pr√©venir l'overfitting.

## Analyses et Enseignements Cl√©s

### 1. La Batch Normalization : Le "Game Changer"
C'est l'optimisation la plus marquante. Avant la BN, la convergence √©tait lente et instable. 
- **Impact** : La pr√©cision est pass√©e d'environ 55% √† **plus de 76%** en seulement 10 √©poques.
- **D√©marrage rapide** : Gr√¢ce √† la BN, la perte (Loss) d√©marre beaucoup plus bas (environ 1.6 au lieu du 2.3 th√©orique du hasard), car le mod√®le se stabilise et apprend d√®s les premi√®res it√©rations de l'√©poque 0.

### 2. Le dilemme de la couche 64 (512 ‚Üí 64 ‚Üí 10)
L'ajout d'une couche de r√©flexion suppl√©mentaire a montr√© un transfert de performance int√©ressant :
- **Succ√®s** : La d√©tection des **Chiens** a bondi (atteignant ~75.8%).
- **√âchec** : La d√©tection des **Chats** s'est d√©grad√©e (tombant sous les 50%).
- **Analyse** : La r√©duction √† 64 neurones a cr√©√© un goulot d'√©tranglement informatif. Le mod√®le a privil√©gi√© des caract√©ristiques fortes pour certaines classes au d√©triment des d√©tails plus subtils n√©cessaires √† la distinction des chats.

### 3. Analyse Critique de la Data Augmentation
Mes tests ont r√©v√©l√© que sur CIFAR-10 (images de $32 \times 32$ pixels) :
- La Data Augmentation **ralentit consid√©rablement** le temps de calcul.
- √Ä cette r√©solution, les d√©formations peuvent √™tre "destructrices" et n'apportent pas toujours de gain de pr√©cision pure face √† un mod√®le "propre" stabilis√© par BN.
- Elle reste cependant indispensable pour la **robustesse** face √† des images qui ne seraient pas parfaitement cadr√©es.

## R√©sultats Mod√®le Stabilis√© avec BN en 30 √©poques sans Data Augmentation.
- **Pr√©cision moyenne** : ~82.99 %
- **Performances par classe (Exemples)** :
    - üö¢ **Bateau** : 91.4 %
    - üöõ **Camion** : 85.4 %
    - üêé **Cheval** : 88.0 %
    - üê± **Chat** : 63.0 % (Meilleur score atteint sans le bottleneck trop √©troit)


Le projet est contenu dans le notebook `cifar10.ipynb`. Il inclut un syst√®me de **Checkpointing** qui sauvegarde le meilleur mod√®le rencontr√© :
---
*Projet r√©alis√© dans le cadre d'une exploration personnelle du traitement d'image classique.*
