# Evolution d'un mod√®le CNN sur CIFAR-10 : Apprentissage et Optimisation

## üìå √Ä propos de ce projet
Ce d√©p√¥t regroupe les diff√©rentes √©tapes de cr√©ation et d'optimisation d'un r√©seau de neurones convolutif (CNN) pour la classification d'images (Dataset CIFAR-10).

> **Avertissement important** : Le code contenu dans ce projet n'a pas pour objectif d'√™tre une solution "cl√©s en main" ou parfaitement optimis√©e pour la production. Ce travail a √©t√© r√©alis√© dans un but **purement p√©dagogique**. L'objectif √©tait d'apprendre √† identifier les limites d'un mod√®le simple et d'impl√©menter it√©rativement des techniques d'optimisation pour comprendre leur impact r√©el.

## Le Parcours d'Optimisation
Plut√¥t que de viser 95% de pr√©cision imm√©diatement, j'ai choisi de partir d'une architecture basique pour explorer les concepts suivants :

1. **Mod√®le de base** : Compr√©hension des couches de convolution et de pooling.
2. **Architecture Profonde** : Ajout de filtres (32, 64) et gestion de la perte d'information avec le `padding`.
3. **Data Augmentation** : Lutte contre l'overfitting en "torturant" les donn√©es (rotations, flips, variations de lumi√®re).
4. **Stabilit√© avec la Batch Normalization** : Acc√©l√©ration de la convergence et stabilisation des gradients.
5. **R√©gularisation avec le Dropout** : Am√©lioration de la capacit√© de g√©n√©ralisation du mod√®le.

## R√©sultats obtenus
Le passage d'un mod√®le simple √† un mod√®le stabilis√© par **Batch Normalization** a permis de passer d'environ **55%** √† plus de **76%** de pr√©cision moyenne sur le set de test, avec des perc√©es significatives sur des classes complexes comme le "Chat" ou l'"Oiseau".

## Technologies utilis√©es
* Python
* PyTorch / Torchvision
* Matplotlib / Seaborn (Visualisation des pertes et matrices de confusion)
